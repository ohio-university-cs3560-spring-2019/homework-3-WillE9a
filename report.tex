\documentclass{article}
\begin{document}
Standard deviation, denoted by the lower-case Greek letter sigma ($\sigma$), is used for statistical analysis. It is frequently used when analyzing Gaussian distributions (or bell curves) and represents the variation, or dispersion in a given set of values. Simply put, it just tells us how far a value is from the normal. Often it is referred to as the uncertainty or standard error.


\[\sigma = \sqrt{\frac{\sum_{i=1}^{N} (x_i - \bar{x})^2}{N}}\]


It has applications in the many fields of mathematics and physics, and it is the sole topic of this short report. This formula is your friend when you plan on writing (or fixing) a program to calculate the standard deviation!

\end{document}
